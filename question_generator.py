# this is a file that has modified and structured code for generating question 

import json
from langchain.prompts import PromptTemplate
from langchain.output_parsers import JsonOutputToolsParser
#langchain_core.output_parsers.json.JsonOutputParser
from langchain_core.output_parsers.json import JsonOutputParser  
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_ollama import OllamaLLM
import os
from dotenv import load_dotenv
load_dotenv()

# using pydantic to structure teh llm output
from pydantic import BaseModel, ValidationError , Field

# define all the pydantic classes for teh llm structured output
class TestCase(BaseModel):
    """ this is a class that defines how each test case needs to be defined """
    input: str = Field(..., description="The input to the test case ,  this defined the input data that is given to the function")
    expected_output: str = Field(..., description="The expected output of the test case , this defined the data and its type or structure that needs to be outputted by the function ")
    explanation: str = Field(..., description="A brief explanation of why this is the expected output for the given input")

# now define the class for the entire question 
class CodeQuestion(BaseModel):
    """ This class is to give a structure for the entire question that will be generated by the llm"""

    title: str = Field(..., description="The title of the question , this needs to be a little creative or new")
    description: str = Field(..., description="A detailed description of the question ,  this will act as the problme statement , this needs to have clear technical language ane needs to be 1-2 paras long")
    test_cases: list[TestCase] = Field(..., description="A list of test cases for the question")
    time_complexity : str = Field(..., description="The expected time complexity of the optimal solution for the question")
    input_format: str = Field(..., description="A description of the function's input format.")
    output_format: str = Field(..., description="A description of the function's expected return format.")
    constraints: list[str] = Field(..., description="A list of constraints on the input data.")

class QuestionGenerator:

    llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    temperature=0,
    max_tokens=None,
    timeout=None,
    max_retries=2,
    api_key=os.getenv("GEMINI_API_KEY"),
    # other params...
    )

    structured_llm = llm.with_structured_output(CodeQuestion)

    def generate_question(self, json_string_data):
        # in this function define a prompt and then call the gemini llm 

        from langchain_core.prompts import ChatPromptTemplate

        prompt = ChatPromptTemplate.from_messages([
            (
                "system",
                (
                    "You are a principal engineer at a top tech company, known for creating challenging and novel interview problems that test a candidate's deep understanding, not just their memory of standard algorithms."
                    "Your response MUST conform to the provided tool schema.\n\n"
                    "**Your Goal: Generate a TRICKY and DIFFICULT coding problem.**\n\n"
                    "**Key Instructions:**\n"
                    "1. **Obfuscate the Core Concept:** Invent a compelling, real-world narrative that hides the underlying algorithm. The candidate should have to analyze the problem to uncover the required technique; it should NOT be obvious.\n"
                    "2. **Center the Problem on a 'Twist':** You MUST select one of the `key-technical-considerations` from the JSON (e.g., handling duplicates, midpoint overflow, rotated arrays) and make it the central challenge of your problem.\n"
                    "3. **Require Adaptation:** The standard algorithm should not solve the problem directly. The candidate must be required to adapt the algorithm to fit the narrative's unique constraints or goals.\n"
                    "4. **Avoid Trivial Scenarios:** Do not create a problem that is a simple textbook implementation of the algorithm. Create a scenario that feels new and requires genuine thought.\n"
                    "5. **Do Not Name the Algorithm:** Under no circumstances should you mention the name of the algorithm (e.g., 'Binary Search') in the problem description."
                )
            ),
            (
                "user",
                (
                    "Here is the JSON knowledge packet:\n\n```json\n{json_data}\n```\n\n"
                    "Generate the coding question now."
                )
            ),
        ])

        chain = prompt | self.structured_llm

        gemini_response = chain.invoke(
        {
            "json_data": json_string_data
        }
        )

        return gemini_response